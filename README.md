# Multi-modal_Information_Extraction_and_Representation



Here, we are concentrate on collection of research papers relate to information extraction for multi-modal data.   


Table of Contents
=================


<!--   * [Datasets / Shared Tasks](#Datasets_Shared_Tasks) -->
  * [Review on Multi-modal Data Analytics](#Review_on_Multi-modal_Data_Analytics)
  * [Multi-modal Information Extraction from Text](#Multi-modal_Information_Extraction_from_Text)
  * [Multi-modal Representation Learning](#Multi-modal_Representation_Learning)
  * [Multi-modal Entity Linking](#Multi-modal_Entity_Linking)
  * [Multi-modal KG Construction](#Multi-modal_KG_Construction)
  * [Joint Understanding for Text and Image](#Joint_Understanding_for_Text_and_Image)
  * [Multi-modal Knowledge Graphs for Recommender Systems](#Multi-modal_Knowledge_Graphs_for_Recommender_Systems)
  * [Multi-modal Relation Extraction](#Multi-modal_Knowledge_Graphs_for_Recommender_Systems)
  * [Multi-modal Event Extraction](#Multi-modal_Knowledge_Graphs_for_Recommender_Systems)
  * [Tutorials](#Tutorials)


<!-- ## Datasets_Shared_Tasks -->
## Review_on_Multi-modal_Data_Analytics
1. Yang Wang. **Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry and Fusion**. Arxiv 2020. [[Paper]](https://arxiv.org/pdf/2006.08159.pdf) 


2. Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow. **Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods**. Arxiv 2019. [[Paper]](https://arxiv.org/pdf/1907.09358.pdf) 


3. Daheng Wang, Tong Zhao, Wenhao Yu, Nitesh V. Chawla, and Meng Jiang. **Deep Multimodal Complementarity Learning**. TNNLS 2022. [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9758834) 




## Multi-modal_Information_Extraction_from_Text
1. Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, Daniel Napierski, and Marjorie Freedman. **GAIA: A Fine-grained Multimedia Knowledge Extraction System**. ACL 2020. [[Paper]](https://www.aclweb.org/anthology/2020.acl-demos.11.pdf) (Best Demo Paper)


2. Shih-Fu Chang, LP Morency, Alexander Hauptmann, Alberto Del Bimbo, Cathal Gurrin, Hayley Hung, Heng Ji, and Alan Smeaton. **Panel: Challenges for Multimedia/Multimodal Research in the Next Decade**. ACMMM 2019. [[Paper]](https://blender.cs.illinois.edu/paper/multimediapanel.pdf)


3. Manling Li, Ying Lin, Ananya Subburathinam, et al. **GAIA at SM-KBP 2019 - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System**. TACL 2019. [[Paper]](https://blender.cs.illinois.edu/paper/gaia_smkbp_2019.pdf)


4. Tong Xu, Peilun Zhou, and Enhong Chen, Uncertainty in Multimodal Semantic Understanding. **Uncertainty in Multimodal Semantic Understanding**. In Communication of China Association of Artificial Intelligence (in Chinese) 2020. [[Paper]](http://staff.ustc.edu.cn/~tongxu/Papers/CCAAI20.pdf)


5. Tong Xu*, Peilun Zhou*, Linkang Hu, Xiangnan He, Yao Hu, and Enhong Chen. **Socializing the Videos: A Multimodal Approach for Social Relation Recognition**, In ACM Transactions on Multimedia Computing Communications and Applications 2021.  [[Paper]](https://dl.acm.org/doi/abs/10.1145/3416493)



[Relation Extraction in 2018/2019](https://github.com/WindChimeRan/NREPapers2019)


[少样本关系抽取技术](https://zhuanlan.zhihu.com/p/159438322)


[ACL 2020信息抽取方向论文打卡列表（附论文下载）](https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247495324&idx=2&sn=2c840cbdea3771a8ac118a2072871260&chksm=970fc64aa0784f5c9ee1a12c8e5953bb6fe933b0d01ecd5657555f34f6c3b935052d9bc25a02&mpshare=1&scene=1&srcid=0729Xp6FTJfyuM0a5zqm9DoT&sharer_sharetime=1596017808524&sharer_shareid=6a8a89e40ac625725a7e138018e905a5&key=fdd054e9602c88a6ccefb278505958c798ed9ec41215fea92de364e27bdf16fd2ffec59c8a108408ca9a9720b68311bf59d913d3509e2a2cedff36987659030e3396c589bc6b7c349621e84b81a6d0e7&ascene=1&uin=NjI1MjE3OTQy&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=Aa7crhvieAFu6IiEcBk04cQ%3D&pass_ticket=sB%2BOY2Wz8kMm5N9TvFmVlYp6BtrM7A7AVcYZIYR4YzIbaXWHxnkTuYpi2VemZ4%2BE)


[2020年关系抽取相关论文](https://zhuanlan.zhihu.com/p/154492342?utm_source=wechat_session&utm_medium=social&utm_oi=675293261783109632)



## Multi-modal_Representation_Learning
1. Huapeng Xu, Guilin Qi, Jingjing Li, Meng Wang, Kang Xu, and Huan Gao. **Fine-grained Image Classification by Visual-Semantic Embedding**. IJCAI 2018. [[Paper]](https://www.ijcai.org/Proceedings/2018/0145.pdf)


2. Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. **Embedding Multimodal Relational Data for Knowledge Base Completion**. EMNLP 2018. [[Paper]](https://arxiv.org/pdf/1809.01341.pdf) [[Comprehension]](https://blog.csdn.net/dreamweaverccc/article/details/88365241). 


3. Hatem Mousselly-Sergieh, Teresa Botschen, Iryna Gurevych, and Stefan Roth. **A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning**. \*SEM 2018. [[Paper]](https://www.aclweb.org/anthology/S18-2027.pdf). 


4. Ruobing Xie, Zhiyuan Liu, Huanbo Luan, Maosong Sun. **Image-embodied Knowledge Representation Learning**. IJCAI 2017. [[Paper]](https://arxiv.org/pdf/1609.07028v1.pdf). 


5. Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. **Embedding Multimodal Relational Data**. NIPS 2017. [[Paper]](https://www.akbc.ws/2017/papers/26_paper.pdf). 


6. Derong Xu, Tong Xu*, Shiwei Wu, Jingbo Zhou, and Enhong Chen. **Relation-enhanced Negative Sampling for Multimodal Knowledge Graph Completion**. ACM MM 2022. [[Paper]](https://dl.acm.org/doi/10.1145/3503161.3548388). 



[知识图谱之知识表示篇](https://zhuanlan.zhihu.com/p/148785892)

[KDD2020 Tutorial: Multi-modal Network Representation Learning](https://chuxuzhang.github.io/KDD20_Tutorial.html)



## Multi-modal_Entity_Linking
1. Pengfei Luo, Tong Xu*, Shiwei Wu, Chen Zhu, Linli Xu, and Enhong Chen. **Multi-Grained Multimodal Interaction Network for Entity Linking**. KDD 2023. [[Paper]](https://arxiv.org/abs/2307.09721)


[多模态实体链接（Multimodal Entity Linking）论文整理（更新至2023.6.27）](https://zhuanlan.zhihu.com/p/466395379)



## Multi-modal_KG_Construction
1. Meng Wang, Guilin Qi, Haofen Wang, and Qiushuo Zheng. **Richpedia: A Comprehensive Multi-modal Knowledge Graph**. JIST 2019. [[Paper]](https://link.springer.com/content/pdf/10.1007%2F978-3-030-41407-8_9.pdf)


2. Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and David S. Rosenblum. **MMKG: Multi-Modal Knowledge Graphs**. ESWC 2019. [[Paper]](https://arxiv.org/pdf/1903.05485.pdf)


3. Hongzhi Li, Joe Ellis, Heng Ji, and Shih-Fu Chang. **Event Specific Multimodal Pattern Mining for Knowledge Base Construction**. CSME 2018. [[Paper]](https://blender.cs.illinois.edu/paper/acmmm2016.pdf)


4. Sebasti´an Ferrada, Benjamin Bustos, and Aidan Hogan. **IMGpedia: A Linked Dataset with Content-Based Analysis of Wikimedia Images**. ISWC 2017. [[Paper]](https://link.springer.com/content/pdf/10.1007%2F978-3-319-68204-4_8.pdf)


5. Sebasti´an Ferrada, Benjamin Bustos, and Aidan Hogan. **Multimodal Biological Knowledge Graph Completion via Triple Co-attention Mechanism**. ICDE 2023. [[Paper]]()



[多模态知识图谱](https://zhuanlan.zhihu.com/p/163278672?utm_source=wechat_session&utm_medium=social&s_r=1&from=timeline&s_s_i=HwXWYrkvPkQYdtrHVFtqdFjqJ56tpD0iQtyjgfVpiW8%3D) (来自知乎, 漆桂林教授, 东南大学)

[Multimodal Knowledge Graphs: Construction, Inference, and Challenges](http://tcci.ccf.org.cn/conference/2020/dldoc/tutorial_5_2.pdf) (NLPCC 2020) | [多模态知识图谱构建和推理技术](https://hub-cache.baai.ac.cn/hub-pdf/20201111/Tutorial%204%20%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E5%92%8C%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF.pdf)


[知识图谱平台化助力知识图谱行业大发展](https://zhuanlan.zhihu.com/p/159147179)


[同济大学王昊奋：知识图谱在多模态大数据时代的创新和实践 | 世界人工智能大会达观数据论坛](https://mp.weixin.qq.com/s/HNwWVXY1-iP21IdwK7wndg) (来自达观数据, 王昊奋, 同济大学)


[【积微成著】专题分享——多模态知识图谱构建、推理和挑战](https://mp.weixin.qq.com/s/RQwHntpa4-Y7W_cXylYAOw) (来自PlantData知识图谱实战)


[Multi-modal Knowledge Graph](https://github.com/pengfei-luo/multimodal-knowledge-graph#multimodal-knowledge-graph-completion) (来自Github)



## Joint_Understanding_for_Text_and_Image
  



## Multi-modal_Knowledge_Graphs_for_Recommender_Systems
1. Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan Wang, and Kai Zheng. **Multi-modal Knowledge Graphs for Recommender Systems**. CIKM 2020. [[Paper]](https://zheng-kai.com/paper/cikm_2020_sun.pdf)


2. Chuhan Wu, Fangzhao Wu, Tao Qi, Chao Zhang, Yongfeng Huang, and Tong Xu. **MM-Rec: Visiolinguistic Model Empowered Multimodal News Recommendation**. SIGIR 2022. [[Paper]](https://arxiv.org/abs/2104.07407)

## Multi-modal_Relation_Extraction
1. Chen X, Zhang N, Li L, et al. **Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction**[J]. arXiv preprint arXiv:2205.03521, 2022.
2. Hu X, Guo Z, Teng Z, et al. **Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis**[J]. arXiv preprint arXiv:2305.16166, 2023.
3. Liu X, Gao F, Zhang Q, et al. **Graph convolution for multimodal information extraction from visually rich documents**[J]. arXiv preprint arXiv:1903.11279, 2019.
4. Pingali S, Yadav S, Dutta P, et al. **Multimodal graph-based transformer framework for biomedical relation extraction**[J]. arXiv preprint arXiv:2107.00596, 2021.
5. Wan H, Zhang M, Du J, et al. **FL-MSRE: A few-shot learning based approach to multimodal social relation extraction**[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(15): 13916-13923.
6. Wu S, Fei H, Cao Y, et al. **Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling**[J]. arXiv preprint arXiv:2305.11719, 2023.
7. Xu B, Huang S, Du M, et al. **Different data, different modalities! reinforced data splitting for effective multimodal information extraction from social media posts**[C]. Proceedings of the 29th International Conference on Computational Linguistics. 2022: 1855-1864.
8. Zheng C, Feng J, Cai Y, et al. **Rethinking Multimodal Entity and Relation Extraction from a Translation Point of View**[C]. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023: 6810-6824.
9. Zheng C, Feng J, Fu Z, et al. **Multimodal relation extraction with efficient graph alignment**[C]. Proceedings of the 29th ACM International Conference on Multimedia. 2021: 5298-5306.
10. Zheng C, Wu Z, Feng J, et al. **Mnre: A challenge multimodal dataset for neural relation extraction with visual evidence in social media posts**[C]. 2021 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2021: 1-6.


## Multi-modal_Event_Extraction
1. Li M, Xu R, Wang S, et al. **Clip-event: Connecting text and images with event structures**[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16420-16429.
2. Li M, Zareian A, Zeng Q, et al. **Cross-media structured common space for multimedia event extraction**[J]. arXiv preprint arXiv:2005.02472, 2020.
3. Tong M, Wang S, Cao Y, et al. **Image enhanced event detection in news articles**[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 9040-9047.
4. Wan H, Zhang M, Du J, et al. **FL-MSRE: A few-shot learning based approach to multimodal social relation extraction**[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(15): 13916-13923.
5. Zhang L, Zhou D, He Y, et al. **MERL: Multimodal event representation learning in heterogeneous embedding spaces**[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(16): 14420-14427.

## Tutorials
1. Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web. [[ACL 2020]](./tutorials/Multi-modal_Information_Extraction_from_Text.pdf)


2. Manning、 Ostendorf、 Povey、 何晓冬、 周明共话多模态NLP的机遇和挑战（附视频）. [[2020 北京智源大会  圆桌论坛 AI新疆域：多模态自然语言处理前沿趋势]](https://mp.weixin.qq.com/s?__biz=MzU5ODg0MTAwMw==&mid=2247488568&idx=1&sn=d9351b098be46f7bb69d18c6f59ac8a1&chksm=febf57fcc9c8deea89561f083767bd19baa649b2fbade7ab44ec8796a486d08788ce651aec35&mpshare=1&scene=1&srcid=07103OOC4gkqoQKgzyYFtWGN&sharer_sharetime=1594386259878&sharer_shareid=6a8a89e40ac625725a7e138018e905a5&key=b208b7ed0c58a19a9cdc2102a56d53caafab1f92eadca0b95197fda2b01425f0d321d6a7e2fa2fec28d910492ff301dd02853658fa611b4d3a4ba5c65896190e09908aed394c61812ba0133d2ec5613b&ascene=1&uin=NjI1MjE3OTQy&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=Adrm2w%2Fw1A3b0l%2Fg6a0g8eI%3D&pass_ticket=AmiZkESIKgJonY79YuRUaupucWvcklXJKVlGtFfjWtQE2bzHF%2BMV47H%2BkilE%2Fq80)
