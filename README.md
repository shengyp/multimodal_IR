# Multi-modal_Information_Extraction_and_Representation



Here, we are concentrate on collection of research papers relate to information extraction for multi-modal data.   


Table of Contents
=================


<!--   * [Datasets / Shared Tasks](#Datasets_Shared_Tasks) -->
  * [Review on Multi-modal Data Analytics](#Review_on_Multi-modal_Data_Analytics)
  * [Multi-modal Information Extraction from Text](#Multi-modal_Information_Extraction_from_Text)
  * [Multi-modal Representation Learning](#Multi-modal_Representation_Learning)
  * [Multi-modal KG Construction](#Multi-modal_KG_Construction)
  * [Joint Understanding for Text and Image](#Joint_Understanding_for_Text_and_Image)
  * [Text Generation based upon Scene Graph](#Text_Generation_based_upon_Scene_Graph)
  * [Multi-modal Knowledge Graphs for Recommender Systems](#Multi-modal_Knowledge_Graphs_for_Recommender_Systems)
  * [Tutorials](#Tutorials)


<!-- ## Datasets_Shared_Tasks -->
## Review_on_Multi-modal_Data_Analytics
1. Yang Wang. [Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry and Fusion](https://arxiv.org/pdf/2006.08159.pdf). Arxiv 2020. [[Paper]](https://arxiv.org/pdf/2006.08159.pdf) 


2. Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow. [Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods](https://arxiv.org/pdf/1907.09358.pdf). Arxiv 2019. [[Paper]](https://arxiv.org/pdf/1907.09358.pdf) 


## Multi-modal_Information_Extraction_from_Text
1. Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, Daniel Napierski, and Marjorie Freedman. [GAIA: A Fine-grained Multimedia Knowledge Extraction System](https://www.aclweb.org/anthology/2020.acl-demos.11.pdf). ACL 2020. [[Paper]](https://www.aclweb.org/anthology/2020.acl-demos.11.pdf) (Best Demo Paper)


2. Shih-Fu Chang, LP Morency, Alexander Hauptmann, Alberto Del Bimbo, Cathal Gurrin, Hayley Hung, Heng Ji, and Alan Smeaton. [Panel: Challenges for Multimedia/Multimodal Research in the Next Decade](https://blender.cs.illinois.edu/paper/multimediapanel.pdf). ACMMM 2019. [[Paper]](https://blender.cs.illinois.edu/paper/multimediapanel.pdf)


3. Manling Li, Ying Lin, Ananya Subburathinam, et al. [GAIA at SM-KBP 2019 - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System](https://blender.cs.illinois.edu/paper/gaia_smkbp_2019.pdf). TACL 2019. [[Paper]](https://blender.cs.illinois.edu/paper/gaia_smkbp_2019.pdf)


[Relation Extraction in 2018/2019](https://github.com/WindChimeRan/NREPapers2019)


[少样本关系抽取技术](https://zhuanlan.zhihu.com/p/159438322)


[ACL 2020信息抽取方向论文打卡列表（附论文下载）](https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247495324&idx=2&sn=2c840cbdea3771a8ac118a2072871260&chksm=970fc64aa0784f5c9ee1a12c8e5953bb6fe933b0d01ecd5657555f34f6c3b935052d9bc25a02&mpshare=1&scene=1&srcid=0729Xp6FTJfyuM0a5zqm9DoT&sharer_sharetime=1596017808524&sharer_shareid=6a8a89e40ac625725a7e138018e905a5&key=fdd054e9602c88a6ccefb278505958c798ed9ec41215fea92de364e27bdf16fd2ffec59c8a108408ca9a9720b68311bf59d913d3509e2a2cedff36987659030e3396c589bc6b7c349621e84b81a6d0e7&ascene=1&uin=NjI1MjE3OTQy&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=Aa7crhvieAFu6IiEcBk04cQ%3D&pass_ticket=sB%2BOY2Wz8kMm5N9TvFmVlYp6BtrM7A7AVcYZIYR4YzIbaXWHxnkTuYpi2VemZ4%2BE)


[2020年关系抽取相关论文](https://zhuanlan.zhihu.com/p/154492342?utm_source=wechat_session&utm_medium=social&utm_oi=675293261783109632)



## Multi-modal_Representation_Learning
1. Huapeng Xu, Guilin Qi, Jingjing Li, Meng Wang, Kang Xu, and Huan Gao. [Fine-grained Image Classification by Visual-Semantic Embedding](https://www.ijcai.org/Proceedings/2018/0145.pdf). IJCAI 2018. [[Paper]](https://www.ijcai.org/Proceedings/2018/0145.pdf)


2. Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. [Embedding Multimodal Relational Data for Knowledge Base Completion](https://arxiv.org/pdf/1809.01341.pdf). EMNLP 2018. [[Paper]](https://arxiv.org/pdf/1809.01341.pdf) [[Comprehension]](https://blog.csdn.net/dreamweaverccc/article/details/88365241). 


3. Hatem Mousselly-Sergieh, Teresa Botschen, Iryna Gurevych, and Stefan Roth. [A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning](https://www.aclweb.org/anthology/S18-2027.pdf). \*SEM 2018. [[Paper]](https://www.aclweb.org/anthology/S18-2027.pdf). 


4. Ruobing Xie, Zhiyuan Liu, Huanbo Luan, Maosong Sun. [Image-embodied Knowledge Representation Learning](https://arxiv.org/pdf/1609.07028v1.pdf). IJCAI 2017. [[Paper]](https://arxiv.org/pdf/1609.07028v1.pdf). 


5. Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. [Embedding Multimodal Relational Data](https://www.akbc.ws/2017/papers/26_paper.pdf). NIPS 2017. [[Paper]](https://www.akbc.ws/2017/papers/26_paper.pdf). 


[知识图谱之知识表示篇](https://zhuanlan.zhihu.com/p/148785892)

[KDD2020 Tutorial: Multi-modal Network Representation Learning](https://chuxuzhang.github.io/KDD20_Tutorial.html)


## Multi-modal_KG_Construction
1. Meng Wang, Guilin Qi, Haofen Wang, and Qiushuo Zheng. [Richpedia: A Comprehensive Multi-modal Knowledge Graph](https://link.springer.com/content/pdf/10.1007%2F978-3-030-41407-8_9.pdf). JIST 2019. [[Paper]](https://link.springer.com/content/pdf/10.1007%2F978-3-030-41407-8_9.pdf)


2. Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and David S. Rosenblum. [MMKG: Multi-Modal Knowledge Graphs](https://arxiv.org/pdf/1903.05485.pdf). ESWC 2019. [[Paper]](https://arxiv.org/pdf/1903.05485.pdf)


3. Hongzhi Li, Joe Ellis, Heng Ji, and Shih-Fu Chang. [Event Specific Multimodal Pattern Mining for Knowledge Base Construction](https://blender.cs.illinois.edu/paper/acmmm2016.pdf). CSME 2018. [[Paper]](https://blender.cs.illinois.edu/paper/acmmm2016.pdf)


4. Sebasti´an Ferrada, Benjamin Bustos, and Aidan Hogan. [IMGpedia: A Linked Dataset with Content-Based Analysis of Wikimedia Images](https://link.springer.com/content/pdf/10.1007%2F978-3-319-68204-4_8.pdf). ISWC 2017. [[Paper]](https://link.springer.com/content/pdf/10.1007%2F978-3-319-68204-4_8.pdf)



[多模态知识图谱](https://zhuanlan.zhihu.com/p/163278672?utm_source=wechat_session&utm_medium=social&s_r=1&from=timeline&s_s_i=HwXWYrkvPkQYdtrHVFtqdFjqJ56tpD0iQtyjgfVpiW8%3D) (来自知乎, 漆桂林教授, 东南大学)


[知识图谱平台化助力知识图谱行业大发展](https://zhuanlan.zhihu.com/p/159147179)


[同济大学王昊奋：知识图谱在多模态大数据时代的创新和实践 | 世界人工智能大会达观数据论坛](https://mp.weixin.qq.com/s/HNwWVXY1-iP21IdwK7wndg) (来自达观数据, 王昊奋, 同济大学)


## Joint_Understanding_for_Text_and_Image
  

## Text_Generation_based_upon_Scene_Graph



## Multi-modal_Knowledge_Graphs_for_Recommender_Systems
1. Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan Wang, and Kai Zheng. [Multi-modal Knowledge Graphs for Recommender Systems](https://zheng-kai.com/paper/cikm_2020_sun.pdf). CIKM 2020. [[Paper]](https://zheng-kai.com/paper/cikm_2020_sun.pdf)


## Tutorials
1. Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web. [[ACL 2020]](./tutorials/Multi-modal_Information_Extraction_from_Text.pdf)


2. Manning、 Ostendorf、 Povey、 何晓冬、 周明共话多模态NLP的机遇和挑战（附视频）. [[2020 北京智源大会  圆桌论坛 AI新疆域：多模态自然语言处理前沿趋势]](https://mp.weixin.qq.com/s?__biz=MzU5ODg0MTAwMw==&mid=2247488568&idx=1&sn=d9351b098be46f7bb69d18c6f59ac8a1&chksm=febf57fcc9c8deea89561f083767bd19baa649b2fbade7ab44ec8796a486d08788ce651aec35&mpshare=1&scene=1&srcid=07103OOC4gkqoQKgzyYFtWGN&sharer_sharetime=1594386259878&sharer_shareid=6a8a89e40ac625725a7e138018e905a5&key=b208b7ed0c58a19a9cdc2102a56d53caafab1f92eadca0b95197fda2b01425f0d321d6a7e2fa2fec28d910492ff301dd02853658fa611b4d3a4ba5c65896190e09908aed394c61812ba0133d2ec5613b&ascene=1&uin=NjI1MjE3OTQy&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=Adrm2w%2Fw1A3b0l%2Fg6a0g8eI%3D&pass_ticket=AmiZkESIKgJonY79YuRUaupucWvcklXJKVlGtFfjWtQE2bzHF%2BMV47H%2BkilE%2Fq80)
