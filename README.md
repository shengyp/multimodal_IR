# Multi-modal_Information_Extraction_and_Representation



Here, we are concentrate on collection of research papers relate to information extraction for multi-modal data.   


Table of Contents
=================


<!--   * [Datasets / Shared Tasks](#Datasets_Shared_Tasks) -->
  * [Review on Multi-modal Data Analytics](#review_on_multi-modal_data_analytics)
  * [Multi-modal Dataset](#multi-modal_dataset)
  * [Multi-modal Named Entity Recognition](#multi-modal_named_entity_recognition)
  * [Multi-modal Relation Extraction](#multi-modal_relation_extraction)
  * [Multi-modal Event Extraction](#multi-modal_event_extraction)  
  * [Multi-modal Representation Learning](#multi-modal_representation_learning)
  * [Multi-modal Entity Alignment](#multi-modal_entity_alignment)
  * [Multi-modal Entity Linking](#multi-modal_entity_linking)
  * [Multi-modal Grounding](#multi-modal_grounding)
  * [Multi-modal KG Construction](#multi-modal_kG_construction)
  * [Joint Understanding for Text and Image](#Joint_understanding_for_text_and_image)
  * [Multi-modal Knowledge Graphs for Recommender Systems](#multi-modal_knowledge_graphs_for_recommender_systems)
  * [Tutorials](#tutorials)



## review_on_multi-modal_data_analytics
1. Yang Wang. **Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry and Fusion**. Arxiv 2020. [[Paper]](https://arxiv.org/pdf/2006.08159.pdf) 


2. Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow. **Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods**. Arxiv 2019. [[Paper]](https://arxiv.org/pdf/1907.09358.pdf) 


3. Daheng Wang, Tong Zhao, Wenhao Yu, Nitesh V. Chawla, and Meng Jiang. **Deep Multimodal Complementarity Learning**. TNNLS 2022. [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9758834) 



<!-- ## Datasets_Shared_Tasks -->
## multi-modal_dataset
1. Zheng C, Wu Z, Feng J, et al. **Mnre: A challenge multimodal dataset for neural relation extraction with visual evidence in social media posts**[C]. 2021 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2021: 1-6. [[Paper]](https://ieeexplore.ieee.org/document/9428274/) 


2. Guozheng Li, Peng Wang, Jiafeng Xie, et al. **FEED：A Chinese Financial Event Extraction Dataset Constructed by Distant Supervisions**[C]. IJCKG 2021. [[Paper]](https://dl.acm.org/doi/10.1145/3502223.3502229) 




## multi-modal_named_entity_recognition


[【必知必懂论文】之多模态命名实体识别](https://mp.weixin.qq.com/s/6RXhbsVXYscjIDX1gP20iw)



## multi-modal_relation_extraction
1. Liu X, Gao F, Zhang Q, et al. **Graph convolution for multimodal information extraction from visually rich documents**[J]. arXiv preprint arXiv:1903.11279, 2019. [[Paper]](https://arxiv.org/abs/1903.11279#:~:text=In%20this%20paper%2C%20we%20introduce%20a%20graph%20convolution,further%20combined%20with%20text%20embeddings%20for%20entity%20extraction.)


2. Shih-Fu Chang, LP Morency, Alexander Hauptmann, Alberto Del Bimbo, Cathal Gurrin, Hayley Hung, Heng Ji, and Alan Smeaton. **Panel: Challenges for Multimedia/Multimodal Research in the Next Decade**. ACMMM 2019. [[Paper]](https://blender.cs.illinois.edu/paper/multimediapanel.pdf)


3. Manling Li, Ying Lin, Ananya Subburathinam, et al. **GAIA at SM-KBP 2019 - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System**. TACL 2019. [[Paper]](https://blender.cs.illinois.edu/paper/gaia_smkbp_2019.pdf)


4. Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, Daniel Napierski, and Marjorie Freedman. **GAIA: A Fine-grained Multimedia Knowledge Extraction System**. ACL 2020. [[Paper]](https://www.aclweb.org/anthology/2020.acl-demos.11.pdf) (Best Demo Paper)


5. Tong Xu, Peilun Zhou, and Enhong Chen, Uncertainty in Multimodal Semantic Understanding. **Uncertainty in Multimodal Semantic Understanding**. In Communication of China Association of Artificial Intelligence (in Chinese) 2020. [[Paper]](http://staff.ustc.edu.cn/~tongxu/Papers/CCAAI20.pdf)



6. Carl Yang, Jieyu Zhang, Haonan Wang, Sha Li, Yu Shi, Myunghwan Kim, Matt Walker, and Jiawei Han. **Relation Learning on Social Networks with Multi-Modal Graph Edge Variational Autoencoders**[C]. WSDM 2020. [[Paper]](https://arxiv.org/abs/1911.05465) [[Code]](https://github.com/yangji9181/RELEARN)



7. Tong Xu*, Peilun Zhou*, Linkang Hu, Xiangnan He, Yao Hu, and Enhong Chen. **Socializing the Videos: A Multimodal Approach for Social Relation Recognition**, In ACM Transactions on Multimedia Computing Communications and Applications 2021.  [[Paper]](https://dl.acm.org/doi/abs/10.1145/3416493)


8. Wan H, Zhang M, Du J, et al. **FL-MSRE: A few-shot learning based approach to multimodal social relation extraction**[C]. AAAI 2021, 35(15): 13916-13923. [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17639)


9. Pingali S, Yadav S, Dutta P, et al. **Multimodal graph-based transformer framework for biomedical relation extraction**[J]. ACL finds 2021. [[Paper]](https://arxiv.org/abs/2107.00596)


10. Zheng C, Feng J, Fu Z, et al. **Multimodal relation extraction with efficient graph alignment**[C]. ACMM. 2021: 5298-5306. [[Paper]](
https://dl.acm.org/doi/10.1145/3474085.3476968)


11. Chen X, Zhang N, Li L, et al. **Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction**[J]. NAACL 2022. [[Paper]](https://aclanthology.org/2022.findings-naacl.121/)


12. Xu B, Huang S, Du M, et al. **Different data, different modalities! reinforced data splitting for effective multimodal information extraction from social media posts**[C]. COLING 2022: 1855-1864. [[Paper]](https://aclanthology.org/2022.coling-1.160/)


13. Revanth Gangi Reddy†, Xilin Rui†, Manling Li, Xudong Lin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mohit Bansal, Avi Sil, Shih-Fu Chang, Alexander Schwing, and Heng Ji. **MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding**[C]. AAAI 2022. [[Paper]](https://arxiv.org/abs/2112.10728) [[Data]](https://github.com/blender-nlp/MuMuQA)


14. Wu S, Fei H, Cao Y, et al. **Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling**[J]. ACL 2023. [[Paper]](https://aclanthology.org/2023.acl-long.823/)


15. Zheng C, Feng J, Cai Y, et al. **Rethinking Multimodal Entity and Relation Extraction from a Translation Point of View**[C]. ACL (Volume 1: Long Papers). 2023: 6810-6824. [[Paper]](https://aclanthology.org/2023.acl-long.823/)


16. Hu X, Guo Z, Teng Z, et al. **Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis**[J]. ACL 2023. [[Paper]](https://aclanthology.org/2023.acl-short.27/)


[Relation Extraction in 2018/2019](https://github.com/WindChimeRan/NREPapers2019)


[少样本关系抽取技术](https://zhuanlan.zhihu.com/p/159438322)


[ACL 2020信息抽取方向论文打卡列表（附论文下载）](https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247495324&idx=2&sn=2c840cbdea3771a8ac118a2072871260&chksm=970fc64aa0784f5c9ee1a12c8e5953bb6fe933b0d01ecd5657555f34f6c3b935052d9bc25a02&mpshare=1&scene=1&srcid=0729Xp6FTJfyuM0a5zqm9DoT&sharer_sharetime=1596017808524&sharer_shareid=6a8a89e40ac625725a7e138018e905a5&key=fdd054e9602c88a6ccefb278505958c798ed9ec41215fea92de364e27bdf16fd2ffec59c8a108408ca9a9720b68311bf59d913d3509e2a2cedff36987659030e3396c589bc6b7c349621e84b81a6d0e7&ascene=1&uin=NjI1MjE3OTQy&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=Aa7crhvieAFu6IiEcBk04cQ%3D&pass_ticket=sB%2BOY2Wz8kMm5N9TvFmVlYp6BtrM7A7AVcYZIYR4YzIbaXWHxnkTuYpi2VemZ4%2BE)


[2020年关系抽取相关论文](https://zhuanlan.zhihu.com/p/154492342?utm_source=wechat_session&utm_medium=social&utm_oi=675293261783109632)




## multi-modal_event_extraction
1. Li M, Zareian A, Zeng Q, et al. **Cross-media structured common space for multimedia event extraction**[J]. ACL 2020. [[Paper]](https://aclanthology.org/2020.acl-main.230/)


3. Tong M, Wang S, Cao Y, et al. **Image enhanced event detection in news articles**[C]. AAAI 2020, 34(05): 9040-9047. [[Paper]](https://tongmeihan1995.github.io/meihan.github.io/research/AAAI2020.pdf)


4. Manling Li*, Alireza Zareian*, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare R. Voss, Dan Napierski, and Marjorie Freedman. **GAIA: A Fine-grained Multimedia Knowledge Extraction System**[C]. ACL 2020. [[Demo]](https://blender.cs.illinois.edu/paper/aidaacl2020demo.pdf) [[http://blender.cs.illinois.edu/software/gaia-ie/]] [[Video]](http://blender.cs.illinois.edu/software/gaia-ie/gaia.mp4) 


5. Wan H, Zhang M, Du J, et al. **FL-MSRE: A few-shot learning based approach to multimodal social relation extraction**[C]. AAAI 2021, 35(15): 13916-13923. [[Paper]](https://tongmeihan1995.github.io/meihan.github.io/research/AAAI2020.pdf)


6. Zhang L, Zhou D, He Y, et al. **MERL: Multimodal event representation learning in heterogeneous embedding spaces**[C]. AAAI 2021, 35(16): 14420-14427. [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17639)


7. Brian Chen, Xudong Lin, Christopher Thomas, Manling Li, Shoya Yoshida, Lovish Chum, Heng Ji, and Shih-Fu Chang. **Joint Multimedia Event Extraction from Video and Article**[C]. EMNLP 2021 Findings. 


8. Li M, Xu R, Wang S, et al. **Clip-event: Connecting text and images with event structures**[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16420-16429. [[Paper]](https://arxiv.org/pdf/2201.05078.pdf)


9. Jian Liu, Yufeng Chen, and Jinan Xu. **Multimedia Event Extraction From News With a Unified Contrastive Learning Framework**[C]. ACMM. [[Paper]](https://dl.acm.org/doi/10.1145/3503161.3548132f)




## multi-modal_representation_learning
1. Huapeng Xu, Guilin Qi, Jingjing Li, Meng Wang, Kang Xu, and Huan Gao. **Fine-grained Image Classification by Visual-Semantic Embedding**. IJCAI 2018. [[Paper]](https://www.ijcai.org/Proceedings/2018/0145.pdf)


2. Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. **Embedding Multimodal Relational Data for Knowledge Base Completion**. EMNLP 2018. [[Paper]](https://arxiv.org/pdf/1809.01341.pdf) [[Comprehension]](https://blog.csdn.net/dreamweaverccc/article/details/88365241). 


3. Hatem Mousselly-Sergieh, Teresa Botschen, Iryna Gurevych, and Stefan Roth. **A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning**. \*SEM 2018. [[Paper]](https://www.aclweb.org/anthology/S18-2027.pdf). 


4. Ruobing Xie, Zhiyuan Liu, Huanbo Luan, Maosong Sun. **Image-embodied Knowledge Representation Learning**. IJCAI 2017. [[Paper]](https://arxiv.org/pdf/1609.07028v1.pdf). 


5. Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. **Embedding Multimodal Relational Data**. NIPS 2017. [[Paper]](https://www.akbc.ws/2017/papers/26_paper.pdf). 


6. Derong Xu, Tong Xu*, Shiwei Wu, Jingbo Zhou, and Enhong Chen. **Relation-enhanced Negative Sampling for Multimodal Knowledge Graph Completion**. ACM MM 2022. [[Paper]](https://dl.acm.org/doi/10.1145/3503161.3548388). 



[知识图谱之知识表示篇](https://zhuanlan.zhihu.com/p/148785892)

[KDD2020 Tutorial: Multi-modal Network Representation Learning](https://chuxuzhang.github.io/KDD20_Tutorial.html)



## multi-modal_entity_alignment
1. Qian Li, Shu Guo, Yangyifei Luo, Cheng Ji, Lihong Wang, Jiawei Sheng, and Jianxin Li. **Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment**. WWW 2023. [[Paper]](https://dl.acm.org/doi/10.1145/3543507.3583328) [[Report]](https://mp.weixin.qq.com/s/3NDJaT807tOcKH4vCDDCYQ)



## multi-modal_entity_linking
1. Pengfei Luo, Tong Xu*, Shiwei Wu, Chen Zhu, Linli Xu, and Enhong Chen. **Multi-Grained Multimodal Interaction Network for Entity Linking**. KDD 2023. [[Paper]](https://arxiv.org/abs/2307.09721)


[多模态实体链接（Multimodal Entity Linking）论文整理（更新至2023.6.27）](https://zhuanlan.zhihu.com/p/466395379)



## multi-modal_grounding
1. Yu Zhou, Sha Li, Manling Li, Xudong Lin, Shih-Fu Chang, Mohit Bansal, and Heng Ji. **Non-Sequential Graph Script Induction via Multimedia Grounding**. ACL 2023. [[Paper]](https://arxiv.org/abs/2305.17542) [[Code]](https://github.com/bryanzhou008/Multimodal-Graph-Script-Learning/)



## multi-modal_kG_construction
1. Meng Wang, Guilin Qi, Haofen Wang, and Qiushuo Zheng. **Richpedia: A Comprehensive Multi-modal Knowledge Graph**. JIST 2019. [[Paper]](https://link.springer.com/content/pdf/10.1007%2F978-3-030-41407-8_9.pdf)


2. Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and David S. Rosenblum. **MMKG: Multi-Modal Knowledge Graphs**. ESWC 2019. [[Paper]](https://arxiv.org/pdf/1903.05485.pdf)


3. Hongzhi Li, Joe Ellis, Heng Ji, and Shih-Fu Chang. **Event Specific Multimodal Pattern Mining for Knowledge Base Construction**. CSME 2018. [[Paper]](https://blender.cs.illinois.edu/paper/acmmm2016.pdf)


4. Sebasti´an Ferrada, Benjamin Bustos, and Aidan Hogan. **IMGpedia: A Linked Dataset with Content-Based Analysis of Wikimedia Images**. ISWC 2017. [[Paper]](https://link.springer.com/content/pdf/10.1007%2F978-3-319-68204-4_8.pdf)


5. Sebasti´an Ferrada, Benjamin Bustos, and Aidan Hogan. **Multimodal Biological Knowledge Graph Completion via Triple Co-attention Mechanism**. ICDE 2023. [[Paper]]()



[多模态知识图谱](https://zhuanlan.zhihu.com/p/163278672?utm_source=wechat_session&utm_medium=social&s_r=1&from=timeline&s_s_i=HwXWYrkvPkQYdtrHVFtqdFjqJ56tpD0iQtyjgfVpiW8%3D) (来自知乎, 漆桂林教授, 东南大学)

[Multimodal Knowledge Graphs: Construction, Inference, and Challenges](http://tcci.ccf.org.cn/conference/2020/dldoc/tutorial_5_2.pdf) (NLPCC 2020) | [多模态知识图谱构建和推理技术](https://hub-cache.baai.ac.cn/hub-pdf/20201111/Tutorial%204%20%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E5%92%8C%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF.pdf)


[知识图谱平台化助力知识图谱行业大发展](https://zhuanlan.zhihu.com/p/159147179)


[同济大学王昊奋：知识图谱在多模态大数据时代的创新和实践 | 世界人工智能大会达观数据论坛](https://mp.weixin.qq.com/s/HNwWVXY1-iP21IdwK7wndg) (来自达观数据, 王昊奋, 同济大学)


[【积微成著】专题分享——多模态知识图谱构建、推理和挑战](https://mp.weixin.qq.com/s/RQwHntpa4-Y7W_cXylYAOw) (来自PlantData知识图谱实战)


[Multi-modal Knowledge Graph](https://github.com/pengfei-luo/multimodal-knowledge-graph#multimodal-knowledge-graph-completion) (来自Github)



## Joint_Understanding_for_Text_and_Image
  



## multi-modal_knowledge_graphs_for_recommender_systems
1. Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan Wang, and Kai Zheng. **Multi-modal Knowledge Graphs for Recommender Systems**. CIKM 2020. [[Paper]](https://zheng-kai.com/paper/cikm_2020_sun.pdf)


2. Chuhan Wu, Fangzhao Wu, Tao Qi, Chao Zhang, Yongfeng Huang, and Tong Xu. **MM-Rec: Visiolinguistic Model Empowered Multimodal News Recommendation**. SIGIR 2022. [[Paper]](https://arxiv.org/abs/2104.07407)



## tutorials
1. Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web. [[ACL 2020]](./tutorials/Multi-modal_Information_Extraction_from_Text.pdf)


2. Manning、 Ostendorf、 Povey、 何晓冬、 周明共话多模态NLP的机遇和挑战（附视频）. [[2020 北京智源大会  圆桌论坛 AI新疆域：多模态自然语言处理前沿趋势]](https://mp.weixin.qq.com/s?__biz=MzU5ODg0MTAwMw==&mid=2247488568&idx=1&sn=d9351b098be46f7bb69d18c6f59ac8a1&chksm=febf57fcc9c8deea89561f083767bd19baa649b2fbade7ab44ec8796a486d08788ce651aec35&mpshare=1&scene=1&srcid=07103OOC4gkqoQKgzyYFtWGN&sharer_sharetime=1594386259878&sharer_shareid=6a8a89e40ac625725a7e138018e905a5&key=b208b7ed0c58a19a9cdc2102a56d53caafab1f92eadca0b95197fda2b01425f0d321d6a7e2fa2fec28d910492ff301dd02853658fa611b4d3a4ba5c65896190e09908aed394c61812ba0133d2ec5613b&ascene=1&uin=NjI1MjE3OTQy&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=Adrm2w%2Fw1A3b0l%2Fg6a0g8eI%3D&pass_ticket=AmiZkESIKgJonY79YuRUaupucWvcklXJKVlGtFfjWtQE2bzHF%2BMV47H%2BkilE%2Fq80)
